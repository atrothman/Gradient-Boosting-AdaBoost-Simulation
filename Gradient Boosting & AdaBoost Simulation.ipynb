{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## load libraries ##\n",
    "####################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(123456789)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Toy Dataset Simulation ##\n",
    "############################\n",
    "def simulate_df(n=100, seed=123456, binary_flag=False):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ## specify dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    ## specify variables L1 through L6\n",
    "    L1_split = 0.52\n",
    "    L2_split = 0.23\n",
    "    L3_split = 0.38\n",
    "    df['L1'] = np.random.choice([0, 1], size=n, replace=True, p=[L1_split, (1-L1_split)])\n",
    "    df['L2'] = np.random.choice([0, 1], size=n, replace=True, p=[L2_split, (1-L2_split)])\n",
    "    df['L3'] = np.random.choice([0, 1], size=n, replace=True, p=[L3_split, (1-L3_split)])\n",
    "    df['L4'] = np.random.normal(0, 1, df.shape[0])\n",
    "    df['L5'] = np.random.normal(0, 0.75, df.shape[0])\n",
    "    df['L6'] = np.random.normal(0, 2, df.shape[0])\n",
    "    \n",
    "    theta_0 = 5.5\n",
    "    theta_1 = 1.28\n",
    "    theta_2 = 0.42\n",
    "    theta_3 = 2.32\n",
    "    theta_4 = -3.15\n",
    "    theta_5 = 3.12\n",
    "    theta_6 = -4.29\n",
    "    theta_7 = -1.23\n",
    "    theta_8 = -10.18\n",
    "    theta_9 = 2.21\n",
    "    theta_10 = 10.3\n",
    "    \n",
    "    if(binary_flag):\n",
    "        Z = theta_0 + (theta_1*df['L1']) + (theta_2*df['L2']) + (theta_3*df['L3']) + (theta_4*df['L4']) + (theta_5*df['L5']) + (theta_6*df['L6']) + (theta_7*df['L2']*df['L4']) + (theta_8*df['L3']*df['L6']) + (theta_9*df['L5']*df['L5']) + (theta_10*np.sin(df['L5']))\n",
    "        p = 1 / (1 + np.exp(-Z))\n",
    "        df['Y'] = np.random.binomial(1, p)\n",
    "        df.loc[df['Y']==0, 'Y'] = -1\n",
    "    else:\n",
    "        df['Y'] = theta_0 + (theta_1*df['L1']) + (theta_2*df['L2']) + (theta_3*df['L3']) + (theta_4*df['L4']) + (theta_5*df['L5']) + (theta_6*df['L6']) + (theta_7*df['L2']*df['L4']) + (theta_8*df['L3']*df['L6']) + (theta_9*df['L5']*df['L5']) + (theta_10*np.sin(df['L5'])) + np.random.normal(0, 0.1, df.shape[0])\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 710652.9617096438\n",
      "Current Loss: 704697.9593626335\n",
      "Current Loss: 687852.7483979535\n",
      "Current Loss: 681537.2980095611\n",
      "Current Loss: 665478.7445945848\n",
      "Current Loss: 659586.5364438506\n",
      "Current Loss: 640014.5760520245\n",
      "Current Loss: 638497.2551319165\n",
      "Current Loss: 618245.6441627132\n",
      "Current Loss: 618203.3564164565\n",
      "Current Loss: 616505.1593726956\n",
      "Current Loss: 598494.8103541064\n",
      "Current Loss: 587837.59235525\n",
      "Current Loss: 579409.3972249589\n",
      "Current Loss: 574747.8100380445\n",
      "Current Loss: 561192.49506229\n",
      "Current Loss: 543560.0029280054\n",
      "Current Loss: 543207.4725473339\n",
      "Current Loss: 536458.6003377549\n",
      "Current Loss: 526373.448554167\n",
      "Current Loss: 506577.88892479986\n",
      "Current Loss: 493605.0976276407\n",
      "Current Loss: 479834.35924008564\n",
      "Current Loss: 478361.1843650935\n",
      "Current Loss: 464437.1331300254\n",
      "Current Loss: 463945.9471018534\n",
      "Current Loss: 453269.89872302505\n",
      "Current Loss: 449970.73316805396\n",
      "Current Loss: 446826.9311030527\n",
      "Current Loss: 436211.3148086326\n",
      "Current Loss: 436021.4632674543\n",
      "Current Loss: 423511.6916293493\n",
      "Current Loss: 411058.68566782656\n",
      "Current Loss: 410399.2462939522\n",
      "Current Loss: 409161.5796391511\n",
      "Current Loss: 398231.46125373326\n",
      "Current Loss: 394070.8494466457\n",
      "Current Loss: 386273.19315253134\n",
      "Current Loss: 382645.0385364533\n",
      "Current Loss: 373851.2621279127\n",
      "Current Loss: 370918.2526136399\n",
      "Current Loss: 365538.64709001774\n",
      "Current Loss: 355426.54952580825\n",
      "Current Loss: 353083.71098980115\n",
      "Current Loss: 352337.3868750716\n",
      "Current Loss: 342215.16077205515\n",
      "Current Loss: 340771.34818047605\n",
      "Current Loss: 331529.5292991767\n",
      "Current Loss: 327673.2149673328\n",
      "Current Loss: 320907.0485950174\n",
      "Current Loss: 314874.15108079\n",
      "Current Loss: 313616.11980521306\n",
      "Current Loss: 313531.8868977779\n",
      "Current Loss: 311651.2943574407\n",
      "Current Loss: 310433.5649730555\n",
      "Current Loss: 310222.6002992387\n",
      "Current Loss: 306928.7658299215\n",
      "Current Loss: 301386.71440590854\n",
      "Current Loss: 298308.7521602121\n",
      "Current Loss: 293059.2049437755\n",
      "Current Loss: 292466.5744830984\n",
      "Current Loss: 284608.24038666394\n",
      "Current Loss: 277469.1553077301\n",
      "Current Loss: 276542.9267533329\n",
      "Current Loss: 269425.6800820965\n",
      "Current Loss: 268612.08205815696\n",
      "Current Loss: 261958.92358206146\n",
      "Current Loss: 260480.8953893196\n",
      "Current Loss: 256816.60963684093\n",
      "Current Loss: 252983.37924431177\n",
      "Current Loss: 247979.06884418655\n",
      "Current Loss: 245338.6618156132\n",
      "Current Loss: 240078.28532187786\n",
      "Current Loss: 237414.25394981133\n",
      "Current Loss: 232659.5911717951\n",
      "Current Loss: 230423.4866053004\n",
      "Current Loss: 228302.78215420188\n",
      "Current Loss: 224274.45731226815\n",
      "Current Loss: 222165.40571310875\n",
      "Current Loss: 218033.36191215253\n",
      "Current Loss: 215639.49412421364\n",
      "Current Loss: 211328.58788784273\n",
      "Current Loss: 209125.54177934228\n",
      "Current Loss: 205657.31868664\n",
      "Current Loss: 203698.77959883076\n",
      "Current Loss: 200848.22148443872\n",
      "Current Loss: 197525.117542555\n",
      "Current Loss: 195061.11691341407\n",
      "Current Loss: 192058.12374388202\n",
      "Current Loss: 189864.90186620178\n",
      "Current Loss: 187078.0263544001\n",
      "Current Loss: 184777.60818684095\n",
      "Current Loss: 181773.61900820473\n",
      "Current Loss: 179336.9559763196\n",
      "Current Loss: 177216.2947504236\n",
      "Current Loss: 175092.7326326893\n",
      "Current Loss: 174797.27449985978\n",
      "Current Loss: 174679.19557337652\n",
      "Current Loss: 171480.42329184682\n",
      "Current Loss: 168657.6937426494\n",
      "Current Loss: 167254.63146783775\n",
      "Current Loss: 164680.32777834227\n",
      "Current Loss: 163216.798917454\n",
      "Current Loss: 162748.92105322247\n",
      "Current Loss: 162377.09715612113\n",
      "Current Loss: 158955.1413815093\n",
      "Current Loss: 158319.56336072035\n",
      "Current Loss: 155422.98682489424\n",
      "Current Loss: 154105.15968249153\n",
      "Current Loss: 151749.4090346368\n",
      "Current Loss: 150827.50374134025\n",
      "Current Loss: 148514.55484743096\n",
      "Current Loss: 147642.9292573004\n",
      "Current Loss: 145407.3063518315\n",
      "Current Loss: 144583.70158011725\n",
      "Current Loss: 142422.6325978036\n",
      "Current Loss: 141644.88429433925\n",
      "Current Loss: 138850.96813634154\n",
      "Current Loss: 138557.22249771372\n",
      "Current Loss: 134319.70139431165\n",
      "Current Loss: 133310.9262369936\n",
      "Current Loss: 131012.70292141888\n",
      "Current Loss: 129557.36628831655\n",
      "Current Loss: 127168.47135486225\n",
      "Current Loss: 126093.0730979048\n",
      "Current Loss: 123656.42984533444\n",
      "Current Loss: 122577.0735760991\n",
      "Current Loss: 120371.78847452092\n",
      "Current Loss: 115098.3553178191\n",
      "Current Loss: 114381.94397602012\n",
      "Current Loss: 113337.22575410122\n",
      "Current Loss: 111842.33098848654\n",
      "Current Loss: 110014.24215881876\n",
      "Current Loss: 107898.36054707285\n",
      "Current Loss: 107101.28315505602\n",
      "Current Loss: 105463.20985496594\n",
      "Current Loss: 104210.0977287725\n",
      "Current Loss: 102461.82611536943\n",
      "Current Loss: 101428.06074067205\n",
      "Current Loss: 100438.94616654358\n",
      "Current Loss: 99343.58130976882\n",
      "Current Loss: 99306.81865181375\n",
      "Current Loss: 99044.12136647818\n",
      "Current Loss: 98963.69346985182\n",
      "Current Loss: 98519.28898989882\n",
      "Current Loss: 96934.27648627457\n",
      "Current Loss: 96502.64209414904\n",
      "Current Loss: 94792.03530764075\n",
      "Current Loss: 94297.38982154179\n",
      "Current Loss: 94282.10461872505\n",
      "Current Loss: 94208.11940557159\n",
      "Current Loss: 92576.95110354449\n",
      "Current Loss: 92205.50603737839\n",
      "Current Loss: 92172.29090058841\n",
      "Current Loss: 91900.4853233818\n",
      "Current Loss: 91605.25643975171\n",
      "Current Loss: 91504.66354720811\n",
      "Current Loss: 91409.70467952869\n",
      "Current Loss: 91357.71622669604\n",
      "Current Loss: 91318.25092279301\n",
      "Current Loss: 91265.76367777243\n",
      "Current Loss: 90942.42559062898\n",
      "Current Loss: 89345.90332492685\n",
      "Current Loss: 88885.31830921283\n",
      "Current Loss: 87261.30019163976\n",
      "Current Loss: 87020.92575410476\n",
      "Current Loss: 85633.9399970391\n",
      "Current Loss: 85351.62050226197\n",
      "Current Loss: 84142.86736871359\n",
      "Current Loss: 83866.6765259523\n",
      "Current Loss: 82692.01493686068\n",
      "Current Loss: 82179.6383049486\n",
      "Current Loss: 81150.47153545036\n",
      "Current Loss: 80550.11252766372\n",
      "Current Loss: 79700.81038180907\n",
      "Current Loss: 79267.13333679232\n",
      "Current Loss: 78071.34105489578\n",
      "Current Loss: 77480.91764109393\n",
      "Current Loss: 76543.97316741994\n",
      "Current Loss: 75646.69140202974\n",
      "Current Loss: 72646.0811207586\n",
      "Current Loss: 71887.84071169721\n",
      "Current Loss: 71520.41567837253\n",
      "Current Loss: 70521.13365457591\n",
      "Current Loss: 69673.38201552024\n",
      "Current Loss: 69660.10907043757\n",
      "Current Loss: 69605.0659051819\n",
      "Current Loss: 68641.80660152895\n",
      "Current Loss: 68578.17142153681\n",
      "Current Loss: 68372.26760187693\n",
      "Current Loss: 68189.01619172284\n",
      "Current Loss: 67722.8914887289\n",
      "Current Loss: 67568.4834093282\n",
      "Current Loss: 67252.45350136021\n",
      "Current Loss: 67162.78141627622\n",
      "Current Loss: 64312.691048224566\n",
      "Current Loss: 63651.70921901028\n",
      "Current Loss: 63129.86460157968\n",
      "Current Loss: 62518.1381064331\n",
      "Current Loss: 62029.72397733654\n",
      "Current Loss: 61414.68342476339\n",
      "Current Loss: 61029.43516236695\n",
      "Current Loss: 59294.44157865299\n",
      "Current Loss: 59222.3559767924\n",
      "Current Loss: 58940.34655397417\n",
      "Current Loss: 58892.2277889231\n",
      "Current Loss: 56773.55514965262\n",
      "Current Loss: 55996.12860791805\n",
      "Current Loss: 55378.40838071994\n",
      "Current Loss: 54765.945374877774\n",
      "Current Loss: 54133.953500139876\n",
      "Current Loss: 53667.60229443321\n",
      "Current Loss: 53295.077723435235\n",
      "Current Loss: 52581.3704856129\n",
      "Current Loss: 52179.52853536984\n",
      "Current Loss: 51658.84748925111\n",
      "Current Loss: 51401.67201693491\n",
      "Current Loss: 51148.17202376724\n",
      "Current Loss: 51125.21043390123\n",
      "Current Loss: 50497.57997691237\n",
      "Current Loss: 50377.53129592726\n",
      "Current Loss: 50177.041791175325\n",
      "Current Loss: 49568.090029949104\n",
      "Current Loss: 49248.530396047354\n",
      "Current Loss: 49001.083150601065\n",
      "Current Loss: 48877.022265449705\n",
      "Current Loss: 48517.308012651534\n",
      "Current Loss: 48282.32415267401\n",
      "Current Loss: 47828.83666332649\n",
      "Current Loss: 47712.86745662138\n",
      "Current Loss: 47202.33775925983\n",
      "Current Loss: 46882.108732536086\n",
      "Current Loss: 46669.59131369792\n",
      "Current Loss: 46260.12566247381\n",
      "Current Loss: 45776.58492199046\n",
      "Current Loss: 45341.277105797824\n",
      "Current Loss: 44964.079463823764\n",
      "Current Loss: 44631.51287676693\n",
      "Current Loss: 44488.337234159735\n",
      "Current Loss: 44311.42904541342\n",
      "Current Loss: 44110.26134637644\n",
      "Current Loss: 43657.07444237673\n",
      "Current Loss: 43240.37528840854\n",
      "Current Loss: 42915.04760988576\n",
      "Current Loss: 42764.34679305476\n",
      "Current Loss: 42463.39803657081\n",
      "Current Loss: 42289.95225066046\n",
      "Current Loss: 42159.26429845132\n",
      "Current Loss: 41463.603286011436\n",
      "Current Loss: 41081.97071568816\n",
      "Current Loss: 40699.61180219203\n",
      "Current Loss: 40304.07260598593\n",
      "Current Loss: 40109.93616185003\n",
      "Current Loss: 40052.72514000551\n",
      "Current Loss: 39261.60159030113\n",
      "Current Loss: 38877.911873232086\n",
      "Current Loss: 38608.85624434981\n",
      "Current Loss: 38488.15143792985\n",
      "Current Loss: 37986.52095152478\n",
      "Current Loss: 37746.46388044377\n",
      "Current Loss: 37499.45542711287\n",
      "Current Loss: 37392.00777941739\n",
      "Current Loss: 37065.38688117135\n",
      "Current Loss: 36973.68174730719\n",
      "Current Loss: 36885.79764109028\n",
      "Current Loss: 36613.47205657739\n",
      "Current Loss: 36273.780618417055\n",
      "Current Loss: 35592.689029032095\n",
      "Current Loss: 35434.214285905604\n",
      "Current Loss: 35028.8251461701\n",
      "Current Loss: 34883.62111873454\n",
      "Current Loss: 34636.77709174025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 34424.97350993459\n",
      "Current Loss: 33582.415921678315\n",
      "Current Loss: 33483.847406923465\n",
      "Current Loss: 33474.053710730936\n",
      "Current Loss: 32444.304678982004\n",
      "Current Loss: 32211.06997158518\n",
      "Current Loss: 32188.34952098816\n",
      "Current Loss: 31874.02188830543\n",
      "Current Loss: 31536.536546463736\n",
      "Current Loss: 31282.083505735696\n",
      "Current Loss: 31104.52653276269\n",
      "Current Loss: 30841.35230974275\n",
      "Current Loss: 30648.834421287087\n",
      "Current Loss: 30577.39050725543\n",
      "Current Loss: 30276.08752336593\n",
      "Current Loss: 30187.735729412958\n",
      "Current Loss: 29972.717394429645\n",
      "Current Loss: 29793.340421400233\n",
      "Current Loss: 29638.53875756256\n",
      "Current Loss: 29510.269755945115\n",
      "Current Loss: 29379.547656950876\n",
      "Current Loss: 29226.9979969405\n",
      "Current Loss: 29207.379156794406\n",
      "Current Loss: 28989.572158810457\n",
      "Current Loss: 28978.054291158565\n",
      "Current Loss: 28766.830405711382\n",
      "Current Loss: 28751.72197492873\n",
      "Current Loss: 28552.825122293332\n",
      "Current Loss: 28539.860716628336\n",
      "Current Loss: 28347.267987849664\n",
      "Current Loss: 28330.878334836703\n",
      "Current Loss: 28149.771263709412\n",
      "Current Loss: 28135.46565413194\n",
      "Current Loss: 27960.07108076937\n",
      "Current Loss: 27840.093386617806\n",
      "Current Loss: 27689.936156891174\n",
      "Current Loss: 27597.10940668674\n",
      "Current Loss: 27359.00738808418\n",
      "Current Loss: 27353.17355162869\n",
      "Current Loss: 26953.949435115544\n",
      "Current Loss: 26934.816764300842\n",
      "Current Loss: 26922.279089390297\n",
      "Current Loss: 26763.57479292369\n",
      "Current Loss: 26746.083586104985\n",
      "Current Loss: 26611.10703891154\n",
      "Current Loss: 26457.106030819785\n",
      "Current Loss: 26388.59260773957\n",
      "Current Loss: 26250.99463574655\n",
      "Current Loss: 26227.16225732734\n",
      "Current Loss: 26090.565353495305\n",
      "Current Loss: 26045.791645705533\n",
      "Current Loss: 25952.41288010027\n",
      "Current Loss: 25850.985172171993\n",
      "Current Loss: 25771.019075935084\n",
      "Current Loss: 25662.831863547966\n",
      "Current Loss: 25436.924650465407\n",
      "Current Loss: 25434.51472930414\n",
      "Current Loss: 25277.194178270656\n",
      "Current Loss: 25210.11017439387\n",
      "Current Loss: 24989.349135486784\n",
      "Current Loss: 24800.908168859456\n",
      "Current Loss: 24762.812698266738\n",
      "Current Loss: 24715.237364344037\n",
      "Current Loss: 24680.4270853935\n",
      "Current Loss: 24656.242677365193\n",
      "Current Loss: 24646.236309319083\n",
      "Current Loss: 24643.11151925269\n",
      "Current Loss: 24508.6400627316\n",
      "Current Loss: 24494.82022527766\n",
      "Current Loss: 24463.638947491116\n",
      "Current Loss: 24354.873268544532\n",
      "Current Loss: 24331.429723456647\n",
      "Current Loss: 24218.240756731117\n",
      "Current Loss: 24115.120912447233\n",
      "Current Loss: 24070.7581166162\n",
      "Current Loss: 24054.51607348083\n",
      "Current Loss: 24040.845693547784\n",
      "Current Loss: 23955.582363011814\n",
      "Current Loss: 23928.75616419123\n",
      "Current Loss: 23805.548462326005\n",
      "Current Loss: 23747.317672037883\n",
      "Current Loss: 23665.743271413332\n",
      "Current Loss: 22352.927226250165\n",
      "Current Loss: 22348.056679482554\n",
      "Current Loss: 20695.506631994078\n",
      "Current Loss: 20659.06787671643\n",
      "Current Loss: 20481.80666413086\n",
      "Current Loss: 20467.064378678533\n",
      "Current Loss: 20284.621462521038\n",
      "Current Loss: 19624.747254300695\n",
      "Current Loss: 19572.538723135087\n",
      "Current Loss: 19141.506666481662\n",
      "Current Loss: 18951.646972067076\n",
      "Current Loss: 18424.61803041694\n",
      "Current Loss: 18366.473520496398\n",
      "Current Loss: 17736.356876899496\n",
      "Current Loss: 17706.174817571715\n",
      "Current Loss: 17227.188897283908\n",
      "Current Loss: 16865.42270416752\n",
      "Current Loss: 16810.439841116444\n",
      "Current Loss: 16506.596288887667\n",
      "Current Loss: 16356.263782615284\n",
      "Current Loss: 16212.42688532882\n",
      "Current Loss: 16197.41286141526\n",
      "Current Loss: 15727.971459044355\n",
      "Current Loss: 14787.315792399737\n",
      "Current Loss: 14759.192694834399\n",
      "Current Loss: 14179.167662125219\n",
      "Current Loss: 14147.87247044769\n",
      "Current Loss: 13400.540488267348\n",
      "Current Loss: 13023.605467058951\n",
      "Current Loss: 12870.465654882068\n",
      "Current Loss: 12854.344187760586\n",
      "Current Loss: 12424.127826184163\n",
      "Current Loss: 12408.28892964266\n",
      "Current Loss: 11942.034666442136\n",
      "Current Loss: 11775.80673368242\n",
      "Current Loss: 11765.392224843712\n",
      "Current Loss: 11468.375260653236\n",
      "Current Loss: 11366.113542270183\n",
      "Current Loss: 11314.818525395367\n",
      "Current Loss: 11050.520586118802\n",
      "Current Loss: 11045.891915136277\n",
      "Current Loss: 10781.551651313253\n",
      "Current Loss: 10512.09576523857\n",
      "Current Loss: 10490.220887459742\n",
      "Current Loss: 10283.342588807076\n",
      "Current Loss: 10194.11381289737\n",
      "Current Loss: 9884.246356028643\n",
      "Current Loss: 9816.736349174515\n",
      "Current Loss: 9665.590324569053\n",
      "Current Loss: 9531.280037880344\n",
      "Current Loss: 9325.581542054751\n",
      "Current Loss: 9316.5816722867\n",
      "Current Loss: 9105.728428706747\n",
      "Current Loss: 8982.780644960045\n",
      "Current Loss: 8978.794020576532\n",
      "Current Loss: 8852.41786105943\n",
      "Current Loss: 8676.07532326811\n",
      "Current Loss: 8477.910523606186\n",
      "Current Loss: 8357.290210728817\n",
      "Current Loss: 8051.078781420788\n",
      "Current Loss: 8041.100419235026\n",
      "Current Loss: 8037.488754879527\n",
      "Current Loss: 7918.075818838193\n",
      "Current Loss: 7808.243424021924\n",
      "Current Loss: 7598.118703790375\n",
      "Current Loss: 7587.721566887897\n",
      "Current Loss: 7447.029028803668\n",
      "Current Loss: 7420.603579306639\n",
      "Current Loss: 7418.47603922773\n",
      "Current Loss: 7245.012278047554\n",
      "Current Loss: 7222.890887448784\n",
      "Current Loss: 6992.39454102284\n",
      "Current Loss: 6924.093974143406\n",
      "Current Loss: 6742.994456886053\n",
      "Current Loss: 6707.489335856518\n",
      "Current Loss: 6522.931348433151\n",
      "Current Loss: 6398.929771941146\n",
      "Current Loss: 6392.38451907996\n",
      "Current Loss: 6280.182250622129\n",
      "Current Loss: 6272.733119545923\n",
      "Current Loss: 6119.164765618919\n",
      "Current Loss: 6087.94622595069\n",
      "Current Loss: 6013.3097013050565\n",
      "Current Loss: 5897.68197516862\n",
      "Current Loss: 5879.525747866782\n",
      "Current Loss: 5807.6090804449395\n",
      "Current Loss: 5723.149982595872\n",
      "Current Loss: 5675.256674153059\n",
      "Current Loss: 5457.518275283154\n",
      "Current Loss: 5436.213682689827\n",
      "Current Loss: 5311.249297748583\n",
      "Current Loss: 5240.6926752560685\n",
      "Current Loss: 5240.594799920712\n",
      "Current Loss: 5124.940027797096\n",
      "Current Loss: 5114.747318529496\n",
      "Current Loss: 5078.439626033915\n",
      "Current Loss: 5031.220314235313\n",
      "Current Loss: 5025.625586095472\n",
      "Current Loss: 4959.87740078495\n",
      "Current Loss: 4946.936603811562\n",
      "Current Loss: 4841.272963908489\n",
      "Current Loss: 4717.160110698896\n",
      "Current Loss: 4713.643724501477\n",
      "Current Loss: 4705.630903214463\n",
      "Current Loss: 4589.178496885968\n",
      "Current Loss: 4529.7333815782085\n",
      "Current Loss: 4523.610513891812\n",
      "Current Loss: 4458.007716696065\n",
      "Current Loss: 4327.340975478822\n",
      "Current Loss: 4306.286745502517\n",
      "Current Loss: 4196.47426137806\n",
      "Current Loss: 4190.808203183805\n",
      "Current Loss: 4029.633581183578\n",
      "Current Loss: 3948.646088488708\n",
      "Current Loss: 3897.964862984119\n",
      "Current Loss: 3847.7955959656792\n",
      "Current Loss: 3686.241285154368\n",
      "Current Loss: 3684.862875601965\n",
      "Current Loss: 3564.3549031475936\n",
      "Current Loss: 3564.3269779035354\n",
      "Current Loss: 3531.0434424553832\n",
      "Current Loss: 3375.905973837736\n",
      "Current Loss: 3374.7475347612417\n",
      "Current Loss: 3301.0551188285367\n",
      "Current Loss: 3151.1872989944277\n",
      "Current Loss: 3071.3511761460845\n",
      "Current Loss: 2929.425270885266\n",
      "Current Loss: 2820.1683130123824\n",
      "Current Loss: 2768.3679775519595\n",
      "Current Loss: 2695.30325978556\n",
      "Current Loss: 2693.3887114769573\n",
      "Current Loss: 2692.308782236067\n",
      "Current Loss: 2619.8723400465606\n",
      "Current Loss: 2618.0416662972325\n",
      "Current Loss: 2615.55749958914\n",
      "Current Loss: 2588.267348658982\n",
      "Current Loss: 2587.5592764203648\n",
      "Current Loss: 2553.7823991148466\n",
      "Current Loss: 2481.9929480389906\n",
      "Current Loss: 2437.994543796508\n",
      "Current Loss: 2289.2719497063213\n",
      "Current Loss: 2219.810518872739\n",
      "Current Loss: 2156.8440789658207\n",
      "Current Loss: 2079.039146924106\n",
      "Current Loss: 2046.973725839315\n",
      "Current Loss: 2046.7259230608536\n",
      "Current Loss: 2022.673509934151\n",
      "Current Loss: 2012.1913599885745\n",
      "Current Loss: 1962.9911622465113\n",
      "Current Loss: 1934.7535213410217\n",
      "Current Loss: 1910.6401640289403\n",
      "Current Loss: 1909.9427011747816\n",
      "Current Loss: 1865.1286186340565\n",
      "Current Loss: 1816.3562029726618\n",
      "Current Loss: 1765.4506239884442\n",
      "Current Loss: 1765.1699362900115\n",
      "Current Loss: 1756.0994214248485\n",
      "Current Loss: 1749.7759551331446\n",
      "Current Loss: 1735.0460095850685\n",
      "Current Loss: 1667.3816157667245\n",
      "Current Loss: 1661.5515224601713\n",
      "Current Loss: 1635.6762900752726\n",
      "Current Loss: 1570.4350904415376\n",
      "Current Loss: 1529.8691876773264\n",
      "Current Loss: 1529.4388826458517\n",
      "Current Loss: 1505.9481106019884\n",
      "Current Loss: 1478.1259499551793\n",
      "Current Loss: 1433.2437316046758\n",
      "Current Loss: 1397.03637116167\n",
      "Current Loss: 1345.0853844159908\n",
      "Current Loss: 1304.199870573367\n",
      "Current Loss: 1279.292753736285\n",
      "Current Loss: 1273.7979731600863\n",
      "Current Loss: 1261.6860409375536\n",
      "Current Loss: 1248.5340331561226\n",
      "Current Loss: 1248.3420315592634\n",
      "Current Loss: 1221.9482706336373\n",
      "Current Loss: 1192.7073681416282\n",
      "Current Loss: 1165.2619848354805\n",
      "Current Loss: 1133.0029509182443\n",
      "Current Loss: 1109.60340316444\n",
      "Current Loss: 1078.5008830530182\n",
      "Current Loss: 1034.2261244892404\n",
      "Current Loss: 991.5185587272906\n",
      "Current Loss: 971.5319987166922\n",
      "Current Loss: 936.1937504235111\n",
      "Current Loss: 935.8287002445354\n",
      "Current Loss: 908.0926914241622\n",
      "Current Loss: 890.0007223086859\n",
      "Current Loss: 875.4080182510002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 875.1473875758797\n",
      "Current Loss: 862.2151120386825\n",
      "Current Loss: 861.9187834415728\n",
      "Current Loss: 846.0317224109516\n",
      "Current Loss: 830.4360781294096\n",
      "Current Loss: 811.1718902752698\n",
      "Current Loss: 795.0431537753853\n",
      "Current Loss: 780.8245888048266\n",
      "Current Loss: 763.323661082025\n",
      "Current Loss: 736.5326487554436\n",
      "Current Loss: 734.0988246381211\n",
      "Current Loss: 702.4028892503886\n",
      "Current Loss: 701.3515339670636\n",
      "Current Loss: 701.1634781191071\n",
      "Current Loss: 694.4217652014555\n",
      "Current Loss: 691.8657951267226\n",
      "Current Loss: 682.7614075280623\n",
      "Current Loss: 671.5924989116075\n",
      "Current Loss: 669.0691637400205\n",
      "Current Loss: 648.7002254709896\n",
      "Current Loss: 641.2230791145174\n",
      "Current Loss: 621.1235092204985\n",
      "Current Loss: 600.9957704077246\n",
      "Current Loss: 597.2913172845314\n",
      "Current Loss: 593.3311729603283\n",
      "Current Loss: 570.9518521194597\n",
      "Current Loss: 551.7980460767684\n",
      "Current Loss: 551.6542750008251\n",
      "Current Loss: 541.2681006778669\n",
      "Current Loss: 527.951580816983\n",
      "Current Loss: 506.4348832920625\n",
      "Current Loss: 502.911973325232\n",
      "Current Loss: 502.85201804626\n",
      "Current Loss: 498.3249804813257\n",
      "Current Loss: 498.2849584021003\n",
      "Current Loss: 495.164672765411\n",
      "Current Loss: 488.10104641941433\n",
      "Current Loss: 474.99939443347756\n",
      "Current Loss: 464.9367083986725\n",
      "Current Loss: 462.21772747994356\n",
      "Current Loss: 448.01306984319564\n",
      "Current Loss: 440.4867767161743\n",
      "Current Loss: 427.8509275132613\n",
      "Current Loss: 422.01294019037164\n",
      "Current Loss: 411.1246871097106\n",
      "Current Loss: 394.51551975605446\n",
      "Current Loss: 390.0730584192441\n",
      "Current Loss: 382.695894438344\n",
      "Current Loss: 378.54855284125387\n",
      "Current Loss: 378.53026892680526\n",
      "Current Loss: 377.8873530084653\n",
      "Current Loss: 377.2496507142033\n",
      "Current Loss: 367.52962036190473\n",
      "Current Loss: 361.55376636684036\n",
      "Current Loss: 355.6948698978786\n",
      "Current Loss: 352.58318776597855\n",
      "Current Loss: 348.1191035057811\n",
      "Current Loss: 348.0972777505793\n",
      "Current Loss: 341.93459195222766\n",
      "Current Loss: 336.1549723308491\n",
      "Current Loss: 330.8993122505042\n",
      "Current Loss: 327.91653180087525\n",
      "Current Loss: 325.49612823544913\n",
      "Current Loss: 320.387396356707\n",
      "Current Loss: 314.6081979067319\n",
      "Current Loss: 306.21844815737734\n",
      "Current Loss: 296.544304457479\n",
      "Current Loss: 286.35856855770334\n",
      "Current Loss: 282.07900998069823\n",
      "Current Loss: 273.01409298590596\n",
      "Current Loss: 268.11596911625657\n",
      "Current Loss: 263.09512440188803\n",
      "Current Loss: 254.91831109037847\n",
      "Current Loss: 249.8688518046379\n",
      "Current Loss: 245.2197279457596\n",
      "Current Loss: 244.05713598522368\n",
      "Current Loss: 237.85596717396197\n",
      "Current Loss: 232.1929372871832\n",
      "Current Loss: 228.77960471137325\n",
      "Current Loss: 219.60690247080512\n",
      "Current Loss: 215.06267246703507\n",
      "Current Loss: 209.88387835173586\n",
      "Current Loss: 207.43060576761502\n",
      "Current Loss: 205.9027183121923\n",
      "Current Loss: 199.86443030002724\n",
      "Current Loss: 197.43353150588734\n",
      "Current Loss: 195.0488434996821\n",
      "Current Loss: 189.87958303373796\n",
      "Current Loss: 183.98511991857623\n",
      "Current Loss: 178.14290866248135\n",
      "Current Loss: 173.0725898690068\n",
      "Current Loss: 168.54825997385666\n",
      "Current Loss: 167.16897571631947\n",
      "Current Loss: 163.10587384783764\n",
      "Current Loss: 160.7783058995924\n",
      "Current Loss: 158.00542507277467\n",
      "Current Loss: 153.9309763987201\n",
      "Current Loss: 150.45338093687104\n",
      "Current Loss: 146.1535836271811\n",
      "Current Loss: 142.9433202669172\n",
      "Current Loss: 140.61049189726748\n",
      "Current Loss: 135.5948523021051\n",
      "Current Loss: 133.6109571942535\n",
      "Current Loss: 132.0250696104948\n",
      "Current Loss: 130.15782513668867\n",
      "Current Loss: 120.66625270067725\n",
      "Current Loss: 114.84356531676669\n",
      "Current Loss: 112.76629042428672\n",
      "Current Loss: 109.98422007143493\n",
      "Current Loss: 107.40871108901166\n",
      "Current Loss: 103.50286571442214\n",
      "Current Loss: 102.45687444828836\n",
      "Current Loss: 98.47052388413566\n",
      "Current Loss: 96.55178613669798\n",
      "Current Loss: 95.66630107422789\n",
      "Current Loss: 93.73802075275006\n",
      "Current Loss: 91.30805760514455\n",
      "Current Loss: 91.17824614987701\n",
      "Current Loss: 89.05023244227381\n",
      "Current Loss: 88.5534538914187\n",
      "Current Loss: 87.34480517177946\n",
      "Current Loss: 85.44187326490878\n",
      "Current Loss: 82.02086597900649\n",
      "Current Loss: 80.84469631039883\n",
      "Current Loss: 78.77937964201644\n",
      "Current Loss: 77.97837119623328\n",
      "Current Loss: 74.57836276036292\n",
      "Current Loss: 74.1720027475893\n",
      "Current Loss: 73.78774284977268\n",
      "Current Loss: 72.52539854153083\n",
      "Current Loss: 71.19707622168409\n",
      "Current Loss: 71.16862622513496\n",
      "Current Loss: 70.76022189228024\n",
      "Current Loss: 70.73939425281598\n",
      "Current Loss: 69.53341903798038\n",
      "Current Loss: 69.51251223864504\n",
      "Current Loss: 67.75740040020109\n",
      "Current Loss: 66.36238191859418\n",
      "Current Loss: 65.92065248561826\n",
      "Current Loss: 62.24277043347679\n",
      "Current Loss: 61.701296881900255\n",
      "Current Loss: 61.00865741038919\n",
      "Current Loss: 60.96068760979572\n",
      "Current Loss: 60.1681301533211\n",
      "Current Loss: 58.91722740034971\n",
      "Current Loss: 58.90937335590413\n",
      "Current Loss: 56.47179207288355\n",
      "Current Loss: 54.64061807673305\n",
      "Current Loss: 53.423975954875566\n",
      "Current Loss: 52.5373702177382\n",
      "Current Loss: 51.61680704167216\n",
      "Current Loss: 50.4790744645483\n",
      "Current Loss: 49.47603439760036\n",
      "Current Loss: 49.47541380680234\n",
      "Current Loss: 49.44198741867127\n",
      "Current Loss: 49.44084586343871\n",
      "Current Loss: 47.96901609802367\n",
      "Current Loss: 46.88905621991464\n",
      "Current Loss: 45.84865453392557\n",
      "Current Loss: 45.23127415711937\n",
      "Current Loss: 44.398474660904306\n",
      "Current Loss: 42.89403281314044\n",
      "Current Loss: 42.72713493034893\n",
      "Current Loss: 41.8569510664817\n",
      "Current Loss: 40.22933244082126\n",
      "Current Loss: 39.60228020147882\n",
      "Current Loss: 38.925188509661304\n",
      "Current Loss: 38.19985632514084\n",
      "Current Loss: 37.966407044915975\n",
      "Current Loss: 37.38727887474758\n",
      "Current Loss: 36.5532034017721\n",
      "Current Loss: 36.289514322677675\n",
      "Current Loss: 35.38991533032069\n",
      "Current Loss: 34.449540379292884\n",
      "Current Loss: 33.749554995797986\n",
      "Current Loss: 33.2482791159283\n",
      "Current Loss: 32.89726726967225\n",
      "Current Loss: 32.2053506441892\n",
      "Current Loss: 31.551782105189183\n",
      "Current Loss: 30.844049463431876\n",
      "Current Loss: 30.18889265493777\n",
      "Current Loss: 29.344888351945546\n",
      "Current Loss: 29.341333577277403\n",
      "Current Loss: 29.103530340504335\n",
      "Current Loss: 29.017068194517705\n",
      "Current Loss: 28.93882892267428\n",
      "Current Loss: 27.532012406530217\n",
      "Current Loss: 26.404503624496513\n",
      "Current Loss: 26.174980949761025\n",
      "Current Loss: 25.4158157858447\n",
      "Current Loss: 24.666950469326014\n",
      "Current Loss: 24.36737774403555\n",
      "Current Loss: 23.67960096698769\n",
      "Current Loss: 23.57683501497512\n",
      "Current Loss: 22.806132169364638\n",
      "Current Loss: 22.25839512430125\n",
      "Current Loss: 21.902910311232624\n",
      "Current Loss: 21.355943304391882\n",
      "Current Loss: 20.555716145393273\n",
      "Current Loss: 20.40780564703714\n",
      "Current Loss: 20.21178881907813\n",
      "Current Loss: 20.12033871042252\n",
      "Current Loss: 20.002511554878904\n",
      "Current Loss: 19.999835759102247\n",
      "Current Loss: 19.231809240545687\n",
      "Current Loss: 18.993625934350003\n",
      "Current Loss: 18.328814907976323\n",
      "Current Loss: 17.800445474950887\n",
      "Current Loss: 17.17490865358689\n",
      "Current Loss: 16.642888252792105\n",
      "Current Loss: 16.34708621603156\n",
      "Current Loss: 16.174616123282444\n",
      "Current Loss: 15.788180590781002\n",
      "Current Loss: 15.271712812340114\n",
      "Current Loss: 14.974877793914638\n",
      "Current Loss: 14.932024120401735\n",
      "Current Loss: 14.39624502050964\n",
      "Current Loss: 13.83847061859675\n",
      "Current Loss: 13.571054673620809\n",
      "Current Loss: 13.567602223899259\n",
      "Current Loss: 13.540905091409092\n",
      "Current Loss: 13.539817015428028\n",
      "Current Loss: 13.379541110679128\n",
      "Current Loss: 13.33322838774091\n",
      "Current Loss: 13.110542161660867\n",
      "Current Loss: 12.854377543290553\n",
      "Current Loss: 12.446127087332913\n",
      "Current Loss: 12.151429644149834\n",
      "Current Loss: 11.719463882138589\n",
      "Current Loss: 11.518914705634407\n",
      "Current Loss: 11.332174928910357\n",
      "Current Loss: 10.874827050184162\n",
      "Current Loss: 10.620223002880172\n",
      "Current Loss: 10.434660017640528\n",
      "Current Loss: 10.150384697877746\n",
      "Current Loss: 9.95223468589554\n",
      "Current Loss: 9.639654725336003\n",
      "Current Loss: 9.54450585368826\n",
      "Current Loss: 9.242595847851026\n",
      "Current Loss: 8.739404150139046\n",
      "Current Loss: 8.651632960261676\n",
      "Current Loss: 8.528697794937901\n",
      "Current Loss: 8.273218679379855\n",
      "Current Loss: 8.27085396303348\n",
      "Current Loss: 8.267155146417789\n",
      "Current Loss: 8.266291577839011\n",
      "Current Loss: 8.263932111695956\n",
      "Current Loss: 8.26111821397621\n",
      "Current Loss: 8.26045601882067\n",
      "Current Loss: 8.25773225240911\n",
      "Current Loss: 8.257116690399375\n",
      "Current Loss: 8.254479916637909\n",
      "Current Loss: 8.253908741495652\n",
      "Current Loss: 8.2513559371856\n",
      "Current Loss: 8.250826999278903\n",
      "Current Loss: 8.248355252294926\n",
      "Current Loss: 8.247866494668138\n",
      "Current Loss: 8.245235426163987\n",
      "Current Loss: 8.245020078569338\n",
      "Current Loss: 8.242381616579872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 8.242284711941187\n",
      "Current Loss: 8.24215594319912\n",
      "Current Loss: 8.239628350987497\n",
      "Current Loss: 8.239444987393718\n",
      "Current Loss: 8.2370760645657\n",
      "Current Loss: 8.236709367119886\n",
      "Current Loss: 8.194228834532975\n",
      "Current Loss: 8.141858100569834\n",
      "Current Loss: 7.945139908658383\n",
      "Current Loss: 7.5749740893925255\n",
      "Current Loss: 7.553219430945573\n",
      "Current Loss: 7.363170008864563\n",
      "Current Loss: 7.1464644024222395\n",
      "Current Loss: 6.99384925714579\n",
      "Current Loss: 6.836158204425765\n",
      "Current Loss: 6.7031606378622905\n",
      "Current Loss: 6.620879427302571\n",
      "Current Loss: 6.507187318387684\n",
      "Current Loss: 6.425745092540402\n",
      "Current Loss: 6.26560561711109\n",
      "Current Loss: 6.145804547708139\n",
      "Current Loss: 6.120287805429225\n",
      "Current Loss: 5.944613467583897\n",
      "Current Loss: 5.731580061563935\n",
      "Current Loss: 5.540359058724032\n",
      "Current Loss: 5.3753315398235655\n",
      "Current Loss: 5.145743665385163\n",
      "Current Loss: 5.145445351284552\n",
      "Current Loss: 5.1049612772408235\n",
      "Current Loss: 5.029171902267406\n",
      "Current Loss: 4.961575705660979\n",
      "Current Loss: 4.917014943734417\n",
      "Current Loss: 4.864067368107107\n",
      "Current Loss: 4.757311365561574\n",
      "Current Loss: 4.636817290074622\n",
      "Current Loss: 4.458776748455182\n",
      "Current Loss: 4.362662055137401\n",
      "Current Loss: 4.226024324477231\n",
      "Current Loss: 4.043098643761613\n",
      "Current Loss: 3.905591723177313\n",
      "Current Loss: 3.84333665330904\n",
      "Current Loss: 3.7936280830220572\n",
      "Current Loss: 3.7925868485849645\n",
      "Current Loss: 3.731380046176046\n",
      "Current Loss: 3.7020353162094684\n",
      "Current Loss: 3.6781734700410027\n",
      "Current Loss: 3.63120372450121\n",
      "Current Loss: 3.5955590151429946\n",
      "Current Loss: 3.574808745867717\n",
      "Current Loss: 3.5393267564340194\n",
      "Current Loss: 3.4552928292685476\n",
      "Current Loss: 3.4265847887456165\n",
      "Current Loss: 3.360532364248682\n",
      "Current Loss: 3.289330462263612\n",
      "Current Loss: 3.2067079922042403\n",
      "Current Loss: 3.174301934403852\n",
      "Current Loss: 2.9871383323188074\n",
      "Current Loss: 2.8307391007440197\n",
      "Current Loss: 2.7230145389477944\n",
      "Current Loss: 2.7224763804039194\n",
      "Current Loss: 2.636776636108417\n",
      "Current Loss: 2.6361428375204987\n",
      "Current Loss: 2.512926385507576\n",
      "Current Loss: 2.5050730600544435\n",
      "Current Loss: 2.4979497573443212\n",
      "Current Loss: 2.4767480320349127\n",
      "Current Loss: 2.436879435729868\n",
      "Current Loss: 2.3892529985893147\n",
      "Current Loss: 2.3229727791796426\n",
      "Current Loss: 2.3086560014051125\n",
      "Current Loss: 2.266522395369063\n",
      "Current Loss: 2.1644264048849235\n",
      "Current Loss: 2.1121344676938985\n",
      "Current Loss: 2.069640124637442\n",
      "Current Loss: 2.0548532825798085\n",
      "Current Loss: 1.992564907972793\n",
      "Current Loss: 1.9391686143334972\n",
      "Current Loss: 1.893725983679576\n",
      "Current Loss: 1.8935605254048928\n",
      "Current Loss: 1.8496576371289397\n",
      "Current Loss: 1.8456570777075358\n",
      "Current Loss: 1.821604059006999\n",
      "Current Loss: 1.7770137673274613\n",
      "Current Loss: 1.7768891304833452\n",
      "Current Loss: 1.7712069794811496\n",
      "Current Loss: 1.7711717563368745\n",
      "Current Loss: 1.7339210803263259\n",
      "Current Loss: 1.7131746809881285\n",
      "Current Loss: 1.6836097680408566\n",
      "Current Loss: 1.6552191989657203\n",
      "Current Loss: 1.625778443897072\n",
      "Current Loss: 1.586969937395636\n",
      "Current Loss: 1.5703543115505454\n",
      "Current Loss: 1.5438035850540364\n",
      "Current Loss: 1.5130050581075503\n",
      "Current Loss: 1.509461660593528\n",
      "Current Loss: 1.4792664380912728\n",
      "Current Loss: 1.4654000092897848\n",
      "Current Loss: 1.4291284308528824\n",
      "Current Loss: 1.4239900078898395\n",
      "Current Loss: 1.4234442319121843\n",
      "Current Loss: 1.4160225849392982\n",
      "Current Loss: 1.4111520576835257\n",
      "Current Loss: 1.3939388605918381\n",
      "Current Loss: 1.3938724940515708\n",
      "Current Loss: 1.352131554203395\n",
      "Current Loss: 1.3061782531872854\n",
      "Current Loss: 1.2591291616385514\n",
      "Current Loss: 1.2582348165582722\n",
      "Current Loss: 1.2186396065661387\n",
      "Current Loss: 1.1841485345001528\n",
      "Current Loss: 1.1544718392550537\n",
      "Current Loss: 1.1456436057659032\n",
      "Current Loss: 1.1421317121133396\n",
      "Current Loss: 1.1067962517178849\n",
      "Current Loss: 1.1022666141925264\n",
      "Current Loss: 1.086818982046385\n",
      "Current Loss: 1.0605169774086962\n",
      "Current Loss: 1.0387436691152685\n",
      "Current Loss: 1.0017150421510148\n",
      "Current Loss: 0.9935744613775223\n",
      "model converged\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "## Gradient Boosting with Continuous Outcome ##\n",
    "###############################################\n",
    "\n",
    "## simulate the dataset with continuous outcome Y\n",
    "df = simulate_df(n=1000, seed=123456)\n",
    "df['Y_original'] = df['Y']\n",
    "\n",
    "## fit Gradient Boosting Model with depth-3 Regression Trees as Weak Learners\n",
    "## continue fitting model until loss function < 1\n",
    "alpha = 100000\n",
    "current_loss = sum((df['Y'])**2)\n",
    "while(current_loss > 1):\n",
    "    model = DecisionTreeRegressor(random_state=0, max_depth=3)\n",
    "    model.fit(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']], df['Y'])\n",
    "    df['Y_hat'] = model.predict(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']])\n",
    "    df['Y_hat_squared'] = df['Y_hat']**2\n",
    "    df['Y_hat_scaled'] = np.sqrt(df['Y_hat_squared'] / df['Y_hat_squared'].sum()) * np.sign(df['Y_hat'])\n",
    "    loss_not_lowered_flag = True\n",
    "    while(loss_not_lowered_flag):\n",
    "        new_loss = sum((df['Y'] - (alpha*df['Y_hat_scaled']))**2)\n",
    "        if(new_loss < current_loss):\n",
    "            loss_not_lowered_flag = False\n",
    "            current_loss = new_loss\n",
    "            print('Current Loss: ' + str(current_loss))\n",
    "            df['Y'] = df['Y'] - (alpha*df['Y_hat_scaled'])\n",
    "        else:\n",
    "            alpha = 0.99*alpha\n",
    "    del model\n",
    "print('model converged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 659.1631057636724\n",
      "Current Loss: 487.7655215433816\n",
      "Current Loss: 385.43015602209204\n",
      "Current Loss: 309.6683993321115\n",
      "Current Loss: 255.5542037237089\n",
      "Current Loss: 218.3147055733867\n",
      "Current Loss: 198.66608204748306\n",
      "Current Loss: 171.9175494217112\n",
      "Current Loss: 151.6839290289567\n",
      "Current Loss: 138.20003547506386\n",
      "Current Loss: 131.45019790958239\n",
      "Current Loss: 126.25445993343435\n",
      "Current Loss: 122.78411083637391\n",
      "Current Loss: 116.59354372531038\n",
      "Current Loss: 108.28050371146264\n",
      "Current Loss: 104.39126436345835\n",
      "Current Loss: 101.02340988337336\n",
      "Current Loss: 97.78481006804952\n",
      "Current Loss: 95.31392819906642\n",
      "Current Loss: 92.54905329566287\n",
      "Current Loss: 87.00935770165633\n",
      "Current Loss: 85.1447829237424\n",
      "Current Loss: 83.34377736632854\n",
      "Current Loss: 81.53898117389382\n",
      "Current Loss: 77.92521488496628\n",
      "Current Loss: 74.76450253594427\n",
      "Current Loss: 71.78607286865689\n",
      "Current Loss: 69.08618323068401\n",
      "Current Loss: 67.14548022606846\n",
      "Current Loss: 66.48569498215035\n",
      "Current Loss: 65.60058832576867\n",
      "Current Loss: 64.95562892117327\n",
      "Current Loss: 64.08074748427364\n",
      "Current Loss: 62.77251964431149\n",
      "Current Loss: 61.57671989700741\n",
      "Current Loss: 59.12390384549784\n",
      "Current Loss: 57.80059735171778\n",
      "Current Loss: 57.62259230392607\n",
      "Current Loss: 57.1103099472552\n",
      "Current Loss: 56.15816964029093\n",
      "Current Loss: 55.207937027300154\n",
      "Current Loss: 54.14920809338544\n",
      "Current Loss: 53.40921740639783\n",
      "Current Loss: 52.98460104214518\n",
      "Current Loss: 52.27588072610328\n",
      "Current Loss: 49.272173927586195\n",
      "Current Loss: 48.406881424655296\n",
      "Current Loss: 45.93216912152811\n",
      "Current Loss: 43.89897455643779\n",
      "Current Loss: 43.08978102981988\n",
      "Current Loss: 41.174522006536826\n",
      "Current Loss: 40.280322394127374\n",
      "Current Loss: 39.58737141336605\n",
      "Current Loss: 38.20825630397851\n",
      "Current Loss: 37.39401636886239\n",
      "Current Loss: 36.27185691815006\n",
      "Current Loss: 35.38640212337221\n",
      "Current Loss: 34.63786318569132\n",
      "Current Loss: 33.62679434610299\n",
      "Current Loss: 33.32168552500471\n",
      "Current Loss: 32.96533992374048\n",
      "Current Loss: 32.11242464096666\n",
      "Current Loss: 31.801411315169776\n",
      "Current Loss: 31.52772166810667\n",
      "Current Loss: 31.31577639100214\n",
      "Current Loss: 30.925649752994886\n",
      "Current Loss: 30.386026711646455\n",
      "Current Loss: 29.799764750765025\n",
      "Current Loss: 29.03615127915239\n",
      "Current Loss: 28.45697370120284\n",
      "Current Loss: 28.22475882624438\n",
      "Current Loss: 27.537102971746172\n",
      "Current Loss: 27.169628161485925\n",
      "Current Loss: 26.82361042345563\n",
      "Current Loss: 26.70727773869104\n",
      "Current Loss: 26.512481322812363\n",
      "Current Loss: 26.358216899115927\n",
      "Current Loss: 26.18992518523894\n",
      "Current Loss: 25.713075439020674\n",
      "Current Loss: 25.13403930990786\n",
      "Current Loss: 24.09468629286915\n",
      "Current Loss: 23.304834376269696\n",
      "results converged, all datapoints correctly classified\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "## AdaBoost with Binary Categorical Outcome {-1, 1} ##\n",
    "######################################################\n",
    "\n",
    "## simulate the dataset with continuous outcome Y\n",
    "df = simulate_df(n=1000, seed=123456, binary_flag=True)\n",
    "df = df.rename(columns={'Y':'Y_original'})\n",
    "df['w'] = df.shape[0]*[1/df.shape[0]]\n",
    "df['Y'] = 0\n",
    "\n",
    "## fit AdaBoost Model with depth-2 Decision Trees as Weak Learners\n",
    "## continue fitting model until all n-1000 observations predicted correctly\n",
    "count = True\n",
    "while(count):\n",
    "    model = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "    model.fit(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']], df['Y_original'], sample_weight=df['w'])\n",
    "    df['Y_hat'] = model.predict(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']])\n",
    "    df.loc[df['Y_hat']==df['Y_original'], 'w'] = 0\n",
    "    epsilon = sum(df['w'])\n",
    "    alpha = 0.5*(np.log((1-epsilon)/epsilon))\n",
    "    \n",
    "    df['Y'] = df['Y'] + (alpha*df['Y_hat'])\n",
    "    \n",
    "    current_loss = sum(np.exp(-df['Y_original']*df['Y']))\n",
    "    psi = np.exp(-df['Y_original']*df['Y'])\n",
    "    df['w'] = psi / current_loss\n",
    "    \n",
    "    print('Current Loss: ' + str(current_loss))\n",
    "    \n",
    "    df['Y_final'] = 1\n",
    "    df.loc[df['Y']<0, 'Y_final'] = -1\n",
    "    \n",
    "    if(df.loc[df['Y_original']!=df['Y_final'], :].shape[0] == 0):\n",
    "        print('results converged, all datapoints correctly classified')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Y_final</th>\n",
       "      <th>-1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y_original</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Y_final      -1    1\n",
       "Y_original          \n",
       "-1          309    0\n",
       " 1            0  691"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df['Y_original'], df['Y_final'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
